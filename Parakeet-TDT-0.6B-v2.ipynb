{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkin8vzf6F0k4yzNQfh3yQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t0mst0ne/Anti-Anti-Spider/blob/master/Parakeet-TDT-0.6B-v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "mwOW7VOH1nLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30556835-935f-4c5f-867e-8478af9a1e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May  6 02:43:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMZnz2oiX4iQ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6868c830-a22b-4817-8287-f45ce9c03123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-fw2arx22\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-fw2arx22\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803706 sha256=70c09dab37d358fc09f4daadd31cf582d50fa8d8c7ebfaed7918c626cb5e8727\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4h8ihh26/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# 第一步 選GPU\n",
        "\n",
        "# 第二步 下載 whisper\n",
        "! pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 第三步 上傳檔案到contents\n",
        "!ls -alh"
      ],
      "metadata": {
        "id": "HTvmgViXX90L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89557f68-72c1-4bfd-f335-c30c0887cc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20M\n",
            "drwxr-xr-x 1 root root 4.0K May  6 02:44  .\n",
            "drwxr-xr-x 1 root root 4.0K May  6 02:40  ..\n",
            "-rw-r--r-- 1 root root  20M May  6 02:45 '40年老舊透天大翻修！工程估價單怎麼看？一日花蓮水電學徒｜【宅水電】 [z9lk3eLSPP0].m4a'\n",
            "drwxr-xr-x 4 root root 4.0K May  2 13:33  .config\n",
            "drwxr-xr-x 1 root root 4.0K May  2 13:33  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#第四步 執行音源到文字\n",
        "\n",
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"large-v2\")\n",
        "result = model.transcribe(\"/content/40年老舊透天大翻修！工程估價單怎麼看？一日花蓮水電學徒｜【宅水電】 [z9lk3eLSPP0].m4a\",language=\"chinese\") #english, chinese\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "yF5c_NIMYKIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5890d9fe-4127-45be-a6ba-50a3f4e25749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.87G/2.87G [01:52<00:00, 27.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "師傅 還記得我們上次去香港喜立德參展嗎對我們有一些熟面孔像德哥 水電爸爸 還有瑞當然我們也認識了一些新的朋友對 其中很重要的就是水電媽媽而且她是真正有實戰經驗的我跟你說 畢竟台灣女性的職人真的算少數不多 真的很多而且她要專業有專業要身高有身高要顏值有顏值讓我們歡迎水電媽媽謝謝歡迎你好 梁師傅好上次在香港已經有見過面了然後這一次也特別就是剛好有機會我們來到花蓮真的 超級歡迎你們來到花蓮她的主場對 主場而且我們昨天吃得太飽了對 昨天晚上太精彩了我是做烤肉講話注意一點我是做烤肉啦這次也要恭喜水電媽媽最近聽說你有開始要開一些新的課程吧對 我最近有跟知識衛星合作然後預計要開一堂線上課程教大家所謂我們用電規劃的步驟還有一些方法DIY的部分就是給新手入門用的師傅 究竟水電媽媽是不是真正的水電職人我們口說無憑聽說今天剛好上午有一個維修的案場我們今天就給梁師傅一個小小的任務當水電媽媽的一日小學徒可以啊 我已經當過德哥的學徒這次來當水電媽媽的學徒還不錯下次再收集一個水電爸爸的學徒對啊然後我們再評比哪一個師傅是最優質的我被他當壞人 好可怕好 那在維修之前呢我們一定要先準備一些材料嘛對那今天就順便帶大家逛一下花蓮最大的水電材料行消市真的超大間的你知道嗎這間已經比戶口的B&Q還要大我覺得那個在花蓮啊對水電有興趣的人真的很幸福因為大部分水電材料行都是一個小門面而已而且有時候人多是不是還要排隊其實你沒有辦法像逛超級市場這樣可以看到所有水電材料就很清楚而且每一個你看一看你還可以拿起來看規格然後這邊還有很多服務人員可以詢問那我們剛剛一進來大家就可以看到整排貨架像這一排就是開關插座都在這邊然後面板也有很多不同可以搭配所以你們真的拿料都是來這一間對對對他不會只是單純走過路客或散客不會不會所以很厲害就是他能服務專業戶然後一般民眾他也願意開放讓大家可以就是來拿料我們今天因為等一下要裝兩個馬桶然後還有換燈那順便帶大家來看一下油泥好了像我們現在呢比較常拿的是這個就是之前我們以前裝馬桶的那種有個大小頭沒錯這個就是降低失敗率而且以前我們油泥是只有上圈而且他這種的有個好處你知道嗎如果他今天用的是4吋的時候油泥量就會不太夠可能會漏一個洞然後會有臭味跑起來這種的失敗率就很低那我們今天就拿這個然後這邊水電師傅常拿的抽水馬達加壓馬達這邊也都有光線槽區對啊這線槽超齊全的新竹我還沒有看過這麼大的線槽等一下那邊電料區你們也可以稍微逛一下因為這邊有電子材料電控也有然後電熱水器這邊也有展示出來給你看順熱式的然後儲熱式的瓦斯熱水器全部都有放這邊就是五金區管料管材的接頭都在這邊施工有時候會遇到用到的一些特殊的料件螺絲有單獨一區喔可以幫我拿六鏟15公分的侃燈嗎我要自然光的好謝謝好好走吧好現在到我們的維修案廠了我今天有一個得力助手梁師傅來幫忙今天一定可以很快就完工我來騙便當今天呢我們的案廠主要是這個燈具顏色有點不一樣像這一盞是已經完全不亮了嘛所以等一下我們要來把這六盞同時換掉還要換兩個馬桶欸它那個馬桶遇到什麼問題啊這個馬桶當初裝的時候沒有很密合下面這個水深就會流到這個磁磚縫一直流出來那我們樓上呢是因為它底座當初打太多水泥了所以整個馬桶座是已經有裂痕了也是要換掉所以等一下呢我們就猜拳好了啊猜拳喔好好好猜贏的人呢裝兒童馬桶然後猜輸的人呢就裝我們標準一般型的大人用的馬桶剪刀石頭布第一次換兒童馬桶嗎呃很久以前換過但不常啦因為兒童馬桶其實跟大人的馬桶是一樣貴的一樣貴一樣貴啊因為一樣的工啊做得要更精緻好先排水它這個的吸力真的好像不太行正常來講它會它要下去啵啵啵啵啵啵啵啵啵啵那如果烙烘啊沒有做好就會有來看一下什麼牌子喔喔喔可以聽到上面的已經在打底座了有沒有聽到聲音裂痕啊喔喔喔有沒有你可以看這個我剛剛都沒有打這邊對不對你看它都是大便有沒有它當初就是沒有密耳凹你看偏心管它當初呢配的時候配管就有問題了它這個上下都黏所以上去之後就會比較密貼好那這樣我們馬桶安裝就完成囉等我們水底桶乾掉之後就可以使用了欸梁師傅還好嗎蛤吃飯了是嗎你裝完了對哎呦速度很快喔哎呀那我先去換燈因為我們還有六盞凱登我這邊也快好了啦好這一種型的凱登以前它沒有那個快插座所以你看它是不是要接線然後綁電話布那如果說你不是本行的你可能沒有常常做你會覺得有困難因為這是硬線1.6的然後這個是花線軟線看一下現在新的燈它就有這個快插座所以我們現在要來裝我們先把這個這個這個這個這個這個這個這個這個這個這個這個這個這個這個這個所以我們等一下這個線剪掉然後撥平嘛所以就可以直接插進去雖然電源已經關掉呢大家還是要當作像在活電覆葉一樣那因為我等一下是要接那個快接頭嘛所以我的撥皮長度也不能太長差不多就是一個Pan級的寬度而已然後它插進去就自動鎖死了對就跟我們現在插座一樣你可以拉一下拉一下OK這樣才是有穩的好那我們就可以把它扣回去這樣真的很方便對然後扣回去你就看一下像這邊我這邊有尷尬你這邊可能拍不到那你就避開腳踩就可以放上去好這樣就完成一個了完成加冕儀式好師傅我們去看一下審核媽媽那邊需要什麼資源好Go師傅我來了哈囉現在要看要不要支援要需要是我現在已經換兩盞我們還有四盞我們趕快解決它然後我們就可以離開這個地方OK那我那個幫我開燈測試一下耶亮很多一樣是六盞燈對不對可是我們亮度是不是跟剛剛還沒有換之前差非常多下午呢要帶大家去看一下我們現在正在裝修中的一個暗場因為很多人都有估價單的問題這個問題常常這個也是我們之前一直很想要談但不知道該怎麼談要很謹慎的去談這些事情好現在呢來到我們翻修中的暗場啦這一間老屋我先簡單講一下大概是70年的房子所以它現在已經44年了那這間房子最大問題就是它漏水問題很嚴重還有電力配置當然不會像現在配的那麼大對可能不服使用了啦對因為以前44年的話那個電氣真的沒有那麼多那入戶線就是八平方而已對那就帶我們進去實際看一下好這個是它原本那個電錶的地方未來預計是至少有4到5個人會回來住所以呢我們把這一棟從一錶提升到兩個電錶因為大家知道我們電費是極具質的嘛先分成一樓跟二樓的電錶未來在計費的時候屋主會比較節省這樣對啊而且其實有時候老房子它的管路比較小比較細也比較少如果你的趁現在房子在整理的時候有專用迴路的時候就好用很多沒錯這間的電管我們幾乎全部重買因為新的格局跟原本舊的完全不一樣完全不一樣對完全不一樣這邊呢來到我們主電箱地方但是我們把牆壁打開了買了一個新的匯流排電箱在裡面把它加大對加大了因為一樓的話除了客廳跟一個房間還有一間無障礙廁所之外我們還有一個功能完整的廚房包含就是我們庭院以後的燈啊全部也都是要從一樓拉出去嘛所以說我們一樓的預留電箱就不能太小然後客廳過來就是廚房大家知道現在我們廚房的小家電啊很方便好用但唯一要注意就是說用電比較高像隨便一個氣炸鍋它的瓦數就差不多1500瓦甚至拉到就1650拉到MAX對就是我們插座的極限了對沒錯我們為了讓它可以保持以後彈性使用的選擇所以我們好幾個插座都幫它是配專用插座那這樣又會有另外一個問題啊因為我們廚房迴路就變得很多嘛廚房通常又是房子裡面最深的地方跟客廳一定是有一個距離的所以如果我們要拉那麼多線回到我們主電箱的話那這樣我們布線啊布管其實數量會很多再加上我們考量到成本的問題所以我們現在還有另外的做法應該說蠻常見的做法比如說它只有廚房要改可是它廚房電力不夠的我們就會先從客廳拉一組分迴路啦到廚房的分電箱讓它那個廚房設備的使用的時候比較穩定比較老的房子因為通常都比較偏狹長沒錯沒錯越狹長的這一招越好用欸師傅我剛剛其實在進來案場之前我跟水煙媽媽聊了一下對她給我一份她的估價單喔估價單是嗎厲害了吧欸師傅我想問一下就以我們這種不太懂的人來說它這樣列的算是詳細嗎雖然它整個的工程裡面它有區分為拆除鐵件泥作跟水電工程大象木已經告訴你了其他的還有木作油漆旅門窗它這個寫的算很細了因為它把很多的工項把它做一個拆分然後它會寫個一四水煙媽媽我想請問你們寫一四是什麼意思一四就是我比較難量化的東西比如說好了我買一個電箱那我如果只是買一個箱子進去然後沒有含線路的話那我是不是可以寫比如說一個電箱那如果說像我電箱附近除了箱子之外又有開關然後又有線路的話我就會把它打包成一個包裹嗎這樣就一包就是一四因為我想問的說像寫一四會不會它就是一個估價單的陷阱如果把它濫用的話就會是陷阱好比說你看它第一項台電電表申請作業費到一樓的入戶現金更新內容加大這中間包含了設計申請送電然後代繳現補費然後它使用的是單三的電表這個動作裡面包含了很多的相次它沒有辦法細拆但是有的人就會把它寫成全市的電系更新儀式這個坑超大一到七項全部又再合併是嗎這可能是二到十幾項全部都合在一起對大家其實也要注意一個因為我有看過有的人給我看估價單它其實很簡單室內水電配置然後45坪然後一坪多少錢它就給你一個總價它是用評算的但是這就是變成說你的內容有沒有討論到位小叮嘛嘛我再額外提一個問題像以廚房來說我看了這張估價單我只有發現到第十二項有洗碗槽冷熱水給水跟排水配置可是廚房應該有一些重電配置就是五跟六的話其實就是我們的主要電箱的部分它要組回路等於是重新買管然後重新拉線的原本這些盒子你看就打這樣啊原本是完全沒有盒子的嘛整個這樣打過來對不對對 這個就是走地上過來的所以整個配置都差很多然後廁所改到那邊去對 廁所那邊原本是那個房間的一部分把它整個打通然後做一間對啊 剛好講到那間浴室我相信大家應該很多人想要知道浴室翻修的一些工況那Soleil媽媽那以這一間浴室為例好了你大概做的項目是什麼十四啊 一樓浴室改配管含洗凍啊因為我們要銜接出去外面那個化糞池嘛然後還有冷熱水我們都要銜接出去所以你連那個管溝你其實都沒有看到原本這些管子是沒有的都是我們整個新配那這間比較特別的是因為它要做無障礙廁所因為那個老人家年紀比較大一點嘛所以你看我們在門的附近都還有再配一個地牌我們淋浴不會去做那個支睡墩然後還有配那個緊急壓扣喔 我站不起來了喔 喔 蛤我站不起來了腳 腳踏板掉了阿嬤你怎麼沒有感覺他這次換重機來了喔 看到了 有感覺了 有感覺了那我們接下來來到我們二樓兩間廁所底下我們有做那個玻璃纖維加強因為一般的話大家比較常看到就是抗裂網而已但這不是最終防水面我們還會有兩層的痰泥下去做防水那因為我們這邊原本舊建物的地方不是廁所所以我們管線也是全部都新拉的然後從下面吊管出去因為呢 如果我這邊不吊管我又要無障礙那個地面厚度又不夠的時候就沒有辦法配啦所以我們只好就往下啟動然後做吊管那其實在這個階段呢大家可以根據那個包商給你的設計圖你可以去對照一下你需要插座的位置有沒有配出來還有就是你要注意你的設備跟你現場的店員有沒有做一個搭配這樣子師傅今天這樣跟著水電媽媽一整天你有什麼心得感想第一個很震撼的就是那個啊消失的那個什麼水電材料行超大的真的是跟根本是水電材料行接的扣子扣接下來我們來到這個地方的時候其實會勾起我很多的回憶啊我踏入水電了以後的第一個水電的整戶的翻修案就跟這個房子長得一模一樣不過我也覺得好在有這樣的一個案場我們可以知道說估價單跟現場實作的案例它之間的對應程度到哪我先說因為我們可以看到水電媽媽這張估價單有些東西只寫一次或者有一些簡單的敘述那可是有前提的大家如果是看文字上敘述會覺得說有一些部分比如說我廁所的配管雖然它那個原本的地方完全不是廁所所以它所有的給水排水然後電力跟燈具的迴路通通都是新配那我可以寫廁所的配管寫一次的原因呢是因為我給客戶出完整的配置規劃圖所以它設備位置該在哪裡然後哪裡有排水我全部通通標的很清楚所以我寫一次之後客戶是很清楚的知道我會給他怎樣的東西所以他當然會給我過啊而且還有一個超級重要的就是造圖施工對我比較是這一派的因為可能有些人可能他還是比較是傳統一點的不一定有到這樣子的同胞那水電媽媽我想請問你那如果像假設是我是消費者我想要做改建那我們怎麼應該跟師傅們溝通第一個一定要找你信任的工班然後來看現場嘛那在這之中呢你必須要先想好你未來希望它變成什麼樣子我們才能根據你的需求去打造符合你要的空間有了確定的空間配置我們才有辦法推出一個合理的估價單所以我覺得事前溝通很重要你要把你想要的東西先盡量跟設計師或是跟你的工班去溝通而且把雙方溝通完的共識具體的用一個介面呈現出來大家才會對這個我們溝通的結果是否很清楚很了解最後我們今天節目有個特別不一樣的地方有一個很重要的儀式感拿好登登水電媽媽居家全攻略的線上課程Banner哇太厲害了吧還有這個宣傳廣告欸超棒的這是我跟知事外星合作的一門那個水電相關的線上課程讓新手也可以勇敢踏出第一步那因為很多客戶就是他有裝修那就會有電力規劃配置的問題就會問我嘛我就不藏私的把這些電力規劃方法跟步驟在課堂上全部透露給你們那我這裡面也會把一些簡單適合初學者DIY項目呢放在課程裡面再來就是我有些朋友他原本是不會去修家裡面壞掉的東西的對結果他開了店之後他就想要學會自己怎麼修了對不然的話你開店面的時候他會有很多的時效性的問題真的很多朋友都是開店之後才開始想要自己DIY這個很重要啦不管你是居家自己DIY或者是你需要搬修這些工程界的經驗是非常重要比如說我們新竹縣市裡面就有幾個很有名的工程蟑螂哇真的假的很有名啊我之前出一集了沒有沒有沒有來來來我這次就這樣我之前第一個名字好沒有我之前有一個客戶他要委託我然後他需要前期規劃那我們是負責電力的部分因為他畢竟一樓是跟房東承租後面有一部分房東想要保留跟增建那當然房東就先叫了他認為可以的統包結果那個就是找到一個蟑螂不只不跟他下車還跟他拖時間這叫工程糾紛不算詐騙啊最後是怎麼樣錢拿不回來你還要擺一桌飯菜給他幾罐洋酒再給他一個紅包幫他請走你後面的工班也不敢進去啊因為你進去的時候就會出事情啊沒有這些經驗你可以少走一些冤枉路課程中會跟大家分享我的實務經驗我覺得這蠻重要的啊像之前我們頻道創立這個初衷就是說希望把這一層面紗打開即便看我們的影片不一定動手做嘛建立觀念是最重要的好那如果對於這個線上課程有興趣的朋友呢在我們影片資訊欄下方有限時優惠再輸入折扣碼還有優惠哦好那以上就是今天宅水電的內容那如果對於我們到花蓮跟水電媽媽拍攝影片到這個老屋改造翻修有任何其他的問題都歡迎在我們影片下方留言知道沒錯好記得每週五晚上九點準時收看YouTube宅水電掰掰欸我很好奇欸你會不會逛這個比逛百貨公司更興奮啊欸我其實不太逛街但是我真的材料行逛的還比較多你都不用看一下梁師傅施工的完工的樣子嗎因為我們剛剛不是已經測試過了嗎好好好彼此信任是嗎下車下車了下車了下車了完全相信沒關係如果有問題的話我會再打電話因為我們這是責任施工我覺得要再做這個有啦有簽名啦上面還畫押了我們那個良心廠商是責任施工嗎\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#result = model.transcribe(\"2024 神免 1 SLE SS.m4a\",language=\"english\")\n"
      ],
      "metadata": {
        "id": "l0lTbUDyJ_MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 存檔 檔名 file.txt\n",
        "with open(r'241026_1413.txt', 'w') as fp:\n",
        "    for x in result['segments']:\n",
        "        # write each item on a new line\n",
        "        fp.write(\"%s\\n\" % x['text'])\n",
        "    print('Done')\n",
        "\n"
      ],
      "metadata": {
        "id": "brMZSaugeYNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c71dab5-0c3f-468a-a544-0ffc1e8a5822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPKDp1UcbQzK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}